1. What problem does serverless computing aim to solve compared to traditional microservice deployment on Kubernetes? Give one example where serverless is clearly better, and one where it may not be.

Serverless computing takes care of infrastructure management so developers don’t have to. With Kubernetes, you spend time setting up clusters, nodes, and scaling rules. Serverless lets you focus only on writing code. For example, a small event-driven function like resizing images when users upload photos to a website works great with serverless. It scales automatically based on upload traffic, and you pay only when the function runs. But for a complex application with consistent high traffic, like a real-time gaming server, serverless might not be the best choice. Cold starts can cause delays when functions need to spin up, and the cost might add up faster than running dedicated Kubernetes nodes full-time.

2. What are the advantages of using a service mesh (like Istio) for managing microservices communication instead of relying only on Kubernetes networking?

Service meshes add a layer of control over how microservices talk to each other. Kubernetes handles basic networking, but Istio gives you fine-grained control. You get features like automatic mTLS encryption, so you don’t have to code that into each service. It also provides built-in observability—metrics and traces for every request between services. This makes it easier to debug issues like which service is causing delays. Plus, traffic management becomes simpler. You can route requests based on rules without changing service code.

3. Explain what a sidecar proxy (such as Envoy in Istio) does. Why is it needed in a service mesh?

A sidecar proxy runs alongside each microservice instance. It acts as a middleman for all network traffic going to and from the service. For example, with Envoy proxies in Istio, every request from Service A to Service B goes through A’s sidecar, then B’s sidecar. This setup lets the service mesh handle tasks like encryption, load balancing, and health checks without the service itself knowing. Sidecars are needed because they keep service code clean—developers don’t have to add networking logic to every service. They also let the service mesh update independently, so you can roll out new features without changing service code.

4. What kind of traffic management features does Istio provide? Give two examples of how they can be useful in production systems.

Istio offers several traffic management features. One is weighted routing, which lets you send a percentage of traffic to different versions of a service. For example, when rolling out a new feature, you might send 10% of users to the new version and keep 90% on the old one. This helps you test changes with real users without risking a full outage. Another feature is circuit breaking. If a service starts failing, the circuit breaker stops sending requests to it after a certain number of failures. This prevents cascading failures—like one bad service bringing down the whole system. It gives the failing service time to recover without affecting other parts of the application.

5. Explain how Knative Serving enables autoscaling for an application. What triggers scaling up and scaling down?

Knative Serving automates scaling based on traffic. It uses a component called the Autoscaler that monitors request rates and concurrency. When more requests come in than the current pods can handle, Knative adds more pods. When traffic drops, it removes unused pods—even down to zero. For example, if a web app gets 100 requests per second, Knative might scale up to 5 pods. If traffic falls to zero, it scales down to zero, so you don’t pay for idle resources. Scaling up usually triggers when request concurrency exceeds a set threshold, while scaling down happens when pods stay underutilized for a certain time.

6. What is the role of Knative Eventing, and how does it support event-driven architectures?

Knative Eventing manages how events flow between services in an event-driven architecture. It lets services produce events without knowing which services will consume them. For example, a user registration service can emit a "user created" event, and any service that needs that information—like a welcome email service—can subscribe to it. Knative Eventing handles the routing and delivery, ensuring events don’t get lost. It also supports different event sources, like Kubernetes events or external systems, and provides a consistent way to work with events across different platforms. This makes it easier to build loosely coupled systems where services can evolve independently.

7. How does Knative leverage Kubernetes primitives to provide a serverless experience? Discuss which components of Kubernetes (e.g., Deployments, Services, Horizontal Pod Autoscaler) are abstracted away and how this abstraction benefits developers.

Knative builds on Kubernetes to hide the complexity of managing infrastructure. It abstracts components like Deployments, Services, and Horizontal Pod Autoscalers. Instead of defining these resources manually, developers use Knative’s simpler resources like Knative Services. For example, when you create a Knative Service, it automatically sets up the Deployment, Service, and HPA for you. This lets developers focus on code rather than Kubernetes configurations. The abstraction also standardizes how services are deployed, so teams can follow consistent patterns without deep Kubernetes knowledge. Plus, Knative adds features like automatic scaling to zero and traffic routing, which would require custom configurations with plain Kubernetes.

8. In KServe, what is the main function of an InferenceService, and how does it simplify deploying ML models?

An InferenceService in KServe wraps up everything needed to serve a machine learning model. It handles the model container, scaling, and networking all in one resource. Instead of setting up separate deployments, services, and scaling rules for each model, you define a single InferenceService. KServe automatically configures the necessary components, like the model server (for TensorFlow, PyTorch, etc.) and the ingress. It also supports features like model versioning and A/B testing through its configuration. This simplifies deploying ML models because you don’t need to worry about the underlying infrastructure—just point to your model file and let KServe handle the rest.

9. In a production ML workflow using KServe, describe how data moves from an incoming HTTP request to a model prediction response. Which layers (Knative, Istio, KServe, Kubernetes) handle which responsibilities, and where could latency bottlenecks occur?

When an HTTP request comes in for a prediction, it first hits Istio’s ingress gateway. Istio routes the request to the appropriate KServe InferenceService. From there, Knative Serving manages the scaling of the model pods if needed. Once the pod is running, KServe’s model server loads the model and processes the request data. The data passes through the model, generates a prediction, and sends the response back through the same path. 

Each layer has specific jobs: Istio handles traffic routing and security, Knative manages scaling and pod lifecycle, KServe runs the model inference, and Kubernetes provides the underlying compute resources. Latency can bottleneck in a few places. Cold starts—when Knative has to spin up a new pod—add delay. The model itself might take time to process large inputs, especially for deep learning models. Istio’s sidecar proxies add a small overhead to each request, which can add up at high traffic. Also, if the model is stored in a remote location, loading it into memory can slow things down.

10. How can Istio’s traffic routing capabilities (e.g., weighted routing, retries, circuit breaking) be used to support canary deployments or A/B testing in Knative or KServe environments? Discuss the pros and cons compared to manual rollout strategies.

Istio’s traffic routing lets you split traffic between different versions of a service, which works well for canary deployments and A/B testing. For a canary deployment in Knative, you might send 5% of traffic to a new service version while keeping 95% on the old one. If the new version performs well, you gradually increase the percentage. For A/B testing in KServe, you could route users to different model versions based on their location or other attributes to compare performance. 

Using Istio for this is easier than manual rollouts because you don’t have to update deployment configurations manually or manage traffic split scripts. Istio provides real-time metrics on how each version performs, so you can make informed decisions about rolling out changes. However, it adds complexity to the infrastructure—you need to manage Istio configurations and understand how traffic flows through the mesh. Manual rollouts might be simpler for small teams or simple applications, but they lack the fine-grained control and observability that Istio offers. With manual strategies, you also risk human error when updating configurations, whereas Istio’s declarative approach is more consistent.